{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fba72b8-ccbc-4c65-ab89-cb1ced566e14",
   "metadata": {},
   "source": [
    "#### ========================================================\n",
    "#### SEND-TIME OPTIMIZATION: Two-Stage Probabilistic + Harmonic F1\n",
    "#### Индустриальный стандарт 2025 года (Sber, Tinkoff, Revolut, Nubank)\n",
    "#### ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fac52a-9932-4859-8ed5-d670e4eae306",
   "metadata": {},
   "source": [
    "#### 1. ИМПОРТЫ + КОНФИГ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d85f8a26-b869-4f94-b758-cbf976a76eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from omegaconf import OmegaConf\n",
    "import datetime\n",
    "import yaml\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31caf9f0-7499-4758-9b09-7e66bcb35d9b",
   "metadata": {},
   "source": [
    "#### 2. FEATURE ENGINEERING ВРЕМЕНИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bd334e7-2bb6-4e13-a3ca-a7ccbb1ed685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df: pd.DataFrame, ts_column: str = 'send_ts') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Создаёт лучшие на 2025 год признаки времени для STO.\n",
    "    \n",
    "    Вход:\n",
    "        df — DataFrame с колонкой send_ts (datetime)\n",
    "    \n",
    "    Почему именно так:\n",
    "        - sin/cos — единственный способ заставить модель понять цикличность:\n",
    "          23:00 и 00:00 — рядом, а не на разных концах шкалы\n",
    "        - Бизнес-флаги (зарплатные дни, выходные) — дают +5–10% к качеству сами по себе\n",
    "    \n",
    "    Выход:\n",
    "        df с новыми колонками: hour_sin, hour_cos, dow_sin, dow_cos, is_weekend, is_payday_zone и т.д.\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print('Преобразование времени в цикличное:')\n",
    "    \n",
    "    df = df.copy()\n",
    "    ts = pd.to_datetime(df[ts_column])\n",
    "    \n",
    "    df['hour'] = ts.dt.hour\n",
    "    df['dow']  = ts.dt.dayofweek          # 0=понедельник, 6=воскресенье\n",
    "    df['day']  = ts.dt.day                # 1–31\n",
    "    \n",
    "    # === Циклические преобразования (критически важно!) ===\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['dow_sin']  = np.sin(2 * np.pi * df['dow']  / 7)\n",
    "    df['dow_cos']  = np.cos(2 * np.pi * df['dow']  / 7)\n",
    "    df['day_sin']  = np.sin(2 * np.pi * df['day']  / 31)\n",
    "    df['day_cos']  = np.cos(2 * np.pi * df['day']  / 31)\n",
    "    \n",
    "    # === Бизнес-флаги (подстраиваются под РФ/СНГ) ===\n",
    "    df['is_weekend'] = (df['dow'] >= 5).astype(int)\n",
    "    \n",
    "    # Зарплатные дни: с 25-го по 5-е — пик активности\n",
    "    df['is_payday_zone'] = df['day'].isin(list(range(25,32)) + list(range(1,6))).astype(int)\n",
    "    \n",
    "    # Предпраздничные / постпраздничные — можно добавить свои даты\n",
    "    # df['is_pre_holiday'] = ts.dt.date.isin([...])\n",
    "    \n",
    "    # Удобные бакеты\n",
    "    df['hour_bucket'] = pd.cut(df['hour'], \n",
    "                               bins=[0, 6, 10, 17, 20, 24], \n",
    "                               labels=['night', 'morning', 'day', 'evening', 'late_evening'],\n",
    "                               include_lowest=True)\n",
    "    print('Done')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc4168-4f39-40dd-a493-81528aab4f1a",
   "metadata": {},
   "source": [
    "#### 3. ОБУЧЕНИЕ МОДЕЛЕЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699367e8-6fd9-48e1-bd1b-b1f4e7c8b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bank_model(X_train, y_train, X_val, y_val, cat_features=[]):\n",
    "    \"\"\"\n",
    "    Модель P(accept_bank = 1 | x, t) — обучается на ВСЕХ данных\n",
    "    \"\"\"\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=2000,\n",
    "        depth=10,\n",
    "        learning_rate=0.03,\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='AUC',\n",
    "        random_seed=42,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=200,\n",
    "        task_type=\"GPU\" if __import__('torch').cuda.is_available() else \"CPU\",\n",
    "        cat_features=cat_features\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        use_best_model=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_user_model(X_train_user, y_train_user, X_val_user, y_val_user, cat_features=[]):\n",
    "    \"\"\"\n",
    "    Модель P(accept_user = 1 | x, t, accept_bank=1) — обучается ТОЛЬКО на bank==1\n",
    "    \"\"\"\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=1500,\n",
    "        depth=8,\n",
    "        learning_rate=0.05,\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='AUC',\n",
    "        random_seed=42,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=200,\n",
    "        task_type=\"GPU\" if __import__('torch').cuda.is_available() else \"CPU\",\n",
    "        cat_features=cat_features\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_user, y_train_user,\n",
    "        eval_set=(X_val_user, y_val_user),\n",
    "        use_best_model=True\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbcdcc-62ad-455c-a731-9f1b9074e299",
   "metadata": {},
   "source": [
    "#### 4. КАЛИБРОВКА ВЕРОЯТНОСТЕЙ (Isotonic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a880332-f0f2-4bd0-9b96-295cbd111a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_model(model, X_calib, y_calib):\n",
    "    \"\"\"\n",
    "    Isotonic Regression — монотонная калибровка, идеально для GBDT.\n",
    "    Работает лучше Platt/temperature scaling на табличных данных.\n",
    "    \"\"\"\n",
    "    raw_probs = model.predict_proba(X_calib)[:, 1]\n",
    "    calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "    calibrator.fit(raw_probs, y_calib)\n",
    "    return calibrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e960a52-08aa-46e9-b11d-b17d8f4e1c40",
   "metadata": {},
   "source": [
    "#### 5. ГАРМОНИЧЕСКИЙ F1 КАК СУРРОГАТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96552c5-d44f-4532-b862-485d784b4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonic_f1(p_bank: float, p_user: float) -> float:\n",
    "    return 2 * p_bank * p_user / (p_bank + p_user + 1e-12)\n",
    "\n",
    "\n",
    "# def harmonic_f1(p_bank: float, p_user: float, eps: float = 1e-8) -> float:\n",
    "#     \"\"\"\n",
    "#     Harmonic F1 = 2 / (1/P + 1/R) при P = R = p_bank * p_user\n",
    "#     Это единственная дифференцируемая и интерпретируемая метрика,\n",
    "#     которая одновременно максимизирует и банк, и юзер.\n",
    "#     \"\"\"\n",
    "#     p_success = p_bank * p_user\n",
    "#     if p_success < eps:\n",
    "#         return 0.0\n",
    "#     return 2 * p_success / (p_bank + p_user + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53192d04-7050-4d91-af35-7e094fd4cd87",
   "metadata": {},
   "source": [
    "#### 6. ПРЕДРАСЧЁТ СЕТКИ ВРЕМЕНИ (168 слотов = 7×24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb86670-0048-41d6-9c73-7f431cb9a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_time_grid():\n",
    "    \"\"\"\n",
    "    Создаёт датафрейм со всеми возможными часовыми слотами недели.\n",
    "    Используется и для предрасчёта, и для inference.\n",
    "    \"\"\"\n",
    "    grid = []\n",
    "    for dow in range(7):\n",
    "        for hour in range(24):\n",
    "            grid.append({'dow': dow, 'hour': hour, 'day': 15})  # день не важен для sin/cos\n",
    "    time_df = pd.DataFrame(grid)\n",
    "    time_df['send_ts'] = pd.Timestamp('2025-01-01')  # заглушка для create_time_features\n",
    "    time_df = time_df.astype('string', errors='ignore')\n",
    "    return time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1af17-f5a1-4bc9-9c68-98e3338421d4",
   "metadata": {},
   "source": [
    "#### 7. ИНФЕРЕНС: выбор лучшего времени для одного клиента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c10f58-da82-4d60-bfb3-72d489cccb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_best_time_for_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    time_grid: pd.DataFrame,\n",
    "    bank_model,\n",
    "    user_model,\n",
    "    bank_calibrator,\n",
    "    user_calibrator,\n",
    "    cfg,\n",
    "    top_k: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Для каждого клиента в df ищет топ-top_k лучших времён отправки.\n",
    "    Возвращает исходный df с новыми колонками.\n",
    "    \"\"\"\n",
    "    # Все фичи из конфига\n",
    "    cat_features = [f for f in cfg.features.categorical if f in df.columns]\n",
    "    num_features = [f for f in cfg.features.numeric if f in df.columns]\n",
    "    time_features = cfg.features.time.all_generated\n",
    "    feature_cols = [f for f in (cat_features + num_features + time_features) if f in df.columns]\n",
    "\n",
    "    # Защита: все категориальные — строки + 'missing'\n",
    "    for col in cat_features:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('string').fillna('missing')\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, client_row in df.iterrows():\n",
    "        # Дублируем клиента на все 168 слотов\n",
    "        candidates = pd.DataFrame([client_row.to_dict()] * len(time_grid))\n",
    "\n",
    "        # Подставляем временные фичи\n",
    "        for col in time_grid.columns:\n",
    "            if col in feature_cols:\n",
    "                candidates[col] = time_grid[col].values\n",
    "\n",
    "        # Защита от NaN и Categorical\n",
    "        for col in cat_features:\n",
    "            if col in candidates.columns:\n",
    "                candidates[col] = candidates[col].astype('string').fillna('missing')\n",
    "\n",
    "        X = candidates[feature_cols]\n",
    "\n",
    "        # Предсказания\n",
    "        p_bank_raw = bank_model.predict_proba(X)[:, 1]\n",
    "        p_user_raw = user_model.predict_proba(X)[:, 1]\n",
    "\n",
    "        p_bank = bank_calibrator.predict(p_bank_raw)\n",
    "        p_user = user_calibrator.predict(p_user_raw)\n",
    "\n",
    "        # Harmonic F1 для каждого слота\n",
    "        scores = np.array([harmonic_f1(pb, pu) for pb, pu in zip(p_bank, p_user)])\n",
    "        best_idx = np.argsort(scores)[-top_k:][::-1]\n",
    "\n",
    "        # Собираем топ-k для текущего клиента\n",
    "        client_results = {}\n",
    "        for rank, i in enumerate(best_idx, 1):\n",
    "            slot = time_grid.iloc[i]\n",
    "            dow = int(slot['dow'])\n",
    "            hour = int(slot['hour'])\n",
    "            dow_name = ['Пн', 'Вт', 'Ср', 'Чт', 'Пт', 'Сб', 'Вс'][dow]\n",
    "\n",
    "            client_results.update({\n",
    "                f'best_time_rank_{rank}_dow': dow,\n",
    "                f'best_time_rank_{rank}_hour': hour,\n",
    "                f'best_time_rank_{rank}_dow_name': dow_name,\n",
    "                f'best_time_rank_{rank}_time_str': f\"{dow_name} {hour:02d}:00\",\n",
    "                f'best_time_rank_{rank}_harmonic_f1': round(float(scores[i]), 5),\n",
    "                f'best_time_rank_{rank}_p_bank': round(float(p_bank[i]), 5),\n",
    "                f'best_time_rank_{rank}_p_user': round(float(p_user[i]), 5),\n",
    "            })\n",
    "\n",
    "        # Добавляем идентификатор клиента (если есть)\n",
    "        if 'client_id' in client_row:\n",
    "            client_results['client_id'] = client_row['client_id']\n",
    "\n",
    "        results.append(client_results)\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Объединяем с исходным df по индексу (или по client_id)\n",
    "    if 'client_id' in df.columns and 'client_id' in result_df.columns:\n",
    "        final_df = df[['client_id']].reset_index(drop=True)\n",
    "        final_df = final_df.merge(result_df, on='client_id', how='left')\n",
    "    else:\n",
    "        final_df = pd.concat([df.reset_index(drop=True), result_df], axis=1)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75913525-fea3-4ed8-b00e-8c7356612234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_best_time(\n",
    "#     client_row: pd.Series,\n",
    "#     time_grid: pd.DataFrame,\n",
    "#     bank_model, user_model,\n",
    "#     bank_calibrator, user_calibrator,\n",
    "#     config,                              # ← твой конфиг (OmegaConf)\n",
    "#     top_k: int = 5\n",
    "# ):\n",
    "#     # Все фичи, которые идут в модель\n",
    "#     all_features = (\n",
    "#         config.features.categorical +\n",
    "#         config.features.numeric +\n",
    "#         config.features.time.all_generated\n",
    "#     )\n",
    "\n",
    "#     # Дублируем клиента\n",
    "#     candidates = pd.DataFrame([client_row.to_dict()] * len(time_grid))\n",
    "\n",
    "#     # Добавляем временные фичи из time_grid (без дублирования колонок)\n",
    "#     time_features_to_add = [\n",
    "#         col for col in config.features.time.all_generated\n",
    "#         if col in time_grid.columns\n",
    "#     ]\n",
    "\n",
    "#     for col in time_features_to_add:\n",
    "#         candidates[col] = time_grid[col].values\n",
    "\n",
    "#     # Защита от NaN и Categorical\n",
    "#     cat_like = config.features.categorical + config.features.time.buckets\n",
    "#     for col in cat_like:\n",
    "#         if col in candidates.columns:\n",
    "#             candidates[col] = candidates[col].astype('string').fillna('missing')\n",
    "\n",
    "#     # Финальный X\n",
    "#     X = candidates[all_features]\n",
    "\n",
    "#     # Предсказания\n",
    "#     p_bank = bank_calibrator.predict(bank_model.predict_proba(X)[:, 1])\n",
    "#     p_user = user_calibrator.predict(user_model.predict_proba(X)[:, 1])\n",
    "\n",
    "#     scores = np.array([harmonic_f1(pb, pu) for pb, pu in zip(p_bank, p_user)])\n",
    "#     best_idx = np.argsort(scores)[-top_k:][::-1]\n",
    "\n",
    "#     results = []\n",
    "#     for i in best_idx:\n",
    "#         slot = time_grid.iloc[i]\n",
    "#         results.append({\n",
    "#             'dow': int(slot['dow']),\n",
    "#             'hour': int(slot['hour']),\n",
    "#             'score': round(float(scores[i]), 4),\n",
    "#             'p_bank': round(float(p_bank[i]), 4),\n",
    "#             'p_user': round(float(p_user[i]), 4),\n",
    "#         })\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0687009-83c6-40c9-b8ac-bbdca9e6305c",
   "metadata": {},
   "source": [
    "#### 8. ПОЛНЫЙ ПАЙПЛАЙН: от данных до модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7932d3b4-60e7-4b57-9aab-15f97f1bef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pipeline(df_path: str = None):\n",
    "    config = OmegaConf.load(\"config.yaml\")\n",
    "    \n",
    "    if df_path != None:\n",
    "        # 1. Загрузка\n",
    "        df = pd.read_parquet(df_path)  # или csv\n",
    "    else:\n",
    "        df = generate_data(5000)\n",
    "\n",
    "    # ← ПЕРЕИМЕНОВЫВАЕМ ТОЛЬКО ТОیار\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        config.rename.send_ts: 'send_ts',\n",
    "        config.rename.accept_bank: 'accept_bank',\n",
    "        config.rename.accept_user: 'accept_user',\n",
    "        config.rename.client_id: 'client_id',\n",
    "    })\n",
    "\n",
    "    # 2. Feature engineering\n",
    "    df = create_time_features(df, 'send_ts')\n",
    "   \n",
    "    # 3. Определяем признаки\n",
    "    #cat_features = ['offer_type', 'channel', 'segment', 'hour_bucket', 'country']  #  категориальные\n",
    "    cat_features = config.features.categorical\n",
    "    cat_features = [c for c in cat_features if c in df.columns]\n",
    "\n",
    "    df = df.astype({col: 'string' for col in df.select_dtypes(include=['category', 'object']).columns})\n",
    "    for col in cat_features:\n",
    "        df[col] = df[col].fillna('missing').astype(str)\n",
    "\n",
    "    all_features_from_config = (\n",
    "        config.features.categorical +\n",
    "        config.features.numeric +\n",
    "        config.features.time.all_generated\n",
    "    )\n",
    "    \n",
    "    # Берём только те, что реально есть в датафрейме (защита от опечаток)\n",
    "    feature_cols = [f for f in all_features_from_config if f in df.columns]\n",
    "    \n",
    "    # А cat_features — только те категориальные, что есть\n",
    "    cat_features = [f for f in config.features.categorical if f in df.columns]\n",
    "\n",
    "    # 4. Разбиение (temporal!)\n",
    "    df = df.sort_values('send_ts')\n",
    "    train_df = df.iloc[:int(0.7*len(df))]\n",
    "    val_df   = df.iloc[int(0.7*len(df)):int(0.85*len(df))]\n",
    "    test_df  = df.iloc[int(0.85*len(df)):]\n",
    "    \n",
    "    # 5. Bank model — на всех\n",
    "    bank_model = train_bank_model(\n",
    "        train_df[feature_cols], train_df['accept_bank'],\n",
    "        val_df[feature_cols],   val_df['accept_bank'],\n",
    "        cat_features\n",
    "    )\n",
    "    \n",
    "    # 6. User model — только где bank==1\n",
    "    train_user = train_df[train_df['accept_bank'] == 1]\n",
    "    val_user   = val_df[val_df['accept_bank'] == 1]\n",
    "    \n",
    "    user_model = train_user_model(\n",
    "        train_user[feature_cols], train_user['accept_user'],\n",
    "        val_user[feature_cols],   val_user['accept_user'],\n",
    "        cat_features\n",
    "    )\n",
    "    \n",
    "    # 7. Калибровка\n",
    "    bank_calibrator = calibrate_model(bank_model, val_df[feature_cols], val_df['accept_bank'])\n",
    "    user_calibrator = calibrate_model(user_model, val_user[feature_cols], val_user['accept_user'])\n",
    "    \n",
    "    # 8. Сетка времени\n",
    "    time_grid = build_time_grid()\n",
    "    \n",
    "    # 9. Пример предсказания для одного клиента\n",
    "    # client_example = test_df.iloc[0]\n",
    "    # best_times = predict_best_time(\n",
    "    #     client_example, time_grid,\n",
    "    #     bank_model, user_model,\n",
    "    #     bank_calibrator, user_calibrator,\n",
    "    #     config        # ← просто передаём конфиг\n",
    "    # )\n",
    "    print(\"Запуск персонализации для всех клиентов в test_df...\")\n",
    "    personalized_df = predict_best_time_for_dataset(\n",
    "        df=test_df.copy(),\n",
    "        time_grid=time_grid,\n",
    "        bank_model=bank_model,\n",
    "        user_model=user_model,\n",
    "        bank_calibrator=bank_calibrator,\n",
    "        user_calibrator=user_calibrator,\n",
    "        cfg=config,\n",
    "        top_k=3\n",
    "    )\n",
    "\n",
    "# Сохраняем результат\n",
    "    personalized_df.to_parquet(\"personalized_send_times.parquet\", index=False)\n",
    "    print(f\"Готово! Сохранено {len(personalized_df)} клиентов с персональным временем\")\n",
    "    print(\"\\nПример:\")\n",
    "    print(personalized_df[[\n",
    "    'client_id',\n",
    "    'best_time_rank_1_time_str',\n",
    "    'best_time_rank_1_harmonic_f1',\n",
    "    'best_time_rank_1_p_bank',\n",
    "    'best_time_rank_1_p_user'\n",
    "        ]].head(10))\n",
    "    \n",
    "    print(\"ТОП-5 лучших времён для клиента:\")\n",
    "    for r in best_times:\n",
    "        print(f\"  День {r['dow']} (пн=0), час {r['hour']:02d}:00 → F1 = {r['score']:.4f} \"\n",
    "              f\"(p_bank={r['p_bank']:.3f}, p_user={r['p_user']:.3f})\")\n",
    "    \n",
    "    return {\n",
    "        'bank_model': bank_model,\n",
    "        'user_model': user_model,\n",
    "        'bank_calibrator': bank_calibrator,\n",
    "        'user_calibrator': user_calibrator,\n",
    "        'time_grid': time_grid,\n",
    "        'feature_cols': feature_cols,\n",
    "        'cat_features': cat_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f7e85-fdf5-4b00-ade7-28992499ad5c",
   "metadata": {},
   "source": [
    "#### 9. ГЕНЕРАЦИЯ ДАННЫХ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c66ff4e-ddca-4934-b8a1-ffcec027bfc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Простой вариант\n",
    "def generate_data(n = 20000):\n",
    "    begin = datetime.datetime(2025, 1, 1, 0, 0, 0).timestamp()\n",
    "    end = datetime.datetime(2025, 11, 27, 14, 30, 0).timestamp()\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'send_ts': np.random.randint(int(begin), int(end), n),\n",
    "        'country': np.random.choice(['uk', 'us', 'de'], n),\n",
    "        'age': np.random.randint(18, 65, n),\n",
    "        'app_intensity': np.random.uniform(0, 10, n),\n",
    "    })\n",
    "    \n",
    "    data['send_ts'] = pd.to_datetime(data['send_ts'], unit='s')\n",
    "    data['day'] = data['send_ts'].dt.day   \n",
    "    \n",
    "    # --- uplift accept_bank на 15-й день для uk ---\n",
    "    data['y_true'] = ((data['day'] == 15)&(data['country']== 'uk')) * 0.7*np.random.random(len(data)) + 0.3\n",
    "    data['y_true'] = data['y_true'].clip(0.01, 0.99)\n",
    "    data['accept_bank'] = np.random.binomial(1, data['y_true'])\n",
    "\n",
    "    # --- uplift accept_user на 21-й день для us ---\n",
    "    data['y_true_2'] = ((data['accept_bank'] == 1)&(data['day'] == 21)&(data['country']== 'us')) * 0.3*np.random.random(len(data)) + 0.7\n",
    "    data['y_true_2'] = data['y_true_2'].clip(0.01, 0.99)\n",
    "    data['accept_user'] = np.random.binomial(1, data['y_true_2'])\n",
    "    \n",
    "\n",
    "    # --- Проверка ---\n",
    "    print('Сгенерированы данные')\n",
    "    print(data.sample(3))\n",
    "    print(data.groupby('day')['accept_bank'].mean()[13:16])\n",
    "    print(data.groupby('day')['accept_user'].mean()[20:23])\n",
    "\n",
    "    # --- Убьем лишнее ---\n",
    "    data = data.drop(columns = ['y_true_2','y_true','day'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53a62ccf-b085-4b64-909c-f1c6380e660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # сложный вариант\n",
    "# n_users = 500000\n",
    "\n",
    "# users = pd.DataFrame({\n",
    "#     'user_id': range(n_users),\n",
    "#     'gender': np.random.choice(['M', 'F'], n_users, p=[0.48, 0.52]),\n",
    "#     'country': np.random.choice(['US', 'UK', 'DE', 'FR', 'RU'], n_users),\n",
    "#     'age': np.random.gamma(5, 5, n_users).astype(int).clip(18, 70),\n",
    "#     'app_intensity': np.random.randint(1, 11, n_users),\n",
    "#     'device': np.random.choice(['ios', 'android'], n_users, p=[0.4, 0.6]),\n",
    "#     'has_kids': np.random.choice([0, 1], n_users, p=[0.7, 0.3]),\n",
    "#     'is_left_handed': np.random.choice([0, 1], n_users, p=[0.9, 0.1]),\n",
    "#     'eye_color': np.random.choice(['brown', 'blue', 'green'], n_users, p=[0.6, 0.3, 0.1]),\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4e84d-d5c3-41cb-9084-0565367894e1",
   "metadata": {},
   "source": [
    "#### ЗАПУСК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f75a8bae-5a26-43f1-933e-8e9e28d8ae43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированы данные\n",
      "                 send_ts country  age  app_intensity  day  y_true  \\\n",
      "4864 2025-05-15 16:54:57      us   64       8.367233   15     0.3   \n",
      "4684 2025-05-18 09:59:14      de   43       6.048550   18     0.3   \n",
      "1819 2025-03-07 11:49:30      de   50       0.334946    7     0.3   \n",
      "\n",
      "      accept_bank  y_true_2  accept_user  \n",
      "4864            0       0.7            1  \n",
      "4684            1       0.7            1  \n",
      "1819            0       0.7            1  \n",
      "day\n",
      "14    0.237838\n",
      "15    0.394444\n",
      "16    0.309942\n",
      "Name: accept_bank, dtype: float64\n",
      "day\n",
      "21    0.710983\n",
      "22    0.693642\n",
      "23    0.691892\n",
      "Name: accept_user, dtype: float64\n",
      "\n",
      "Преобразование времени в цикличное:\n",
      "Done\n",
      "0:\ttest: 0.5153185\tbest: 0.5153185 (0)\ttotal: 193ms\tremaining: 6m 25s\n",
      "100:\ttest: 0.5059217\tbest: 0.5511365 (9)\ttotal: 4.43s\tremaining: 1m 23s\n",
      "200:\ttest: 0.4972868\tbest: 0.5511365 (9)\ttotal: 9.2s\tremaining: 1m 22s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.5511365079\n",
      "bestIteration = 9\n",
      "\n",
      "Shrink model to first 10 iterations.\n",
      "0:\ttest: 0.5694313\tbest: 0.5694313 (0)\ttotal: 37.3ms\tremaining: 55.9s\n",
      "100:\ttest: 0.4905536\tbest: 0.5694313 (0)\ttotal: 3.46s\tremaining: 48s\n",
      "200:\ttest: 0.4606084\tbest: 0.5694313 (0)\ttotal: 7.43s\tremaining: 48s\n",
      "Stopped by overfitting detector  (200 iterations wait)\n",
      "\n",
      "bestTest = 0.5694313244\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "Запуск персонализации для всех клиентов в test_df...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Замени на свой путь\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     models = \u001b[43mfull_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Сохрани всё для продакшена\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mfull_pipeline\u001b[39m\u001b[34m(df_path)\u001b[39m\n\u001b[32m     82\u001b[39m     personalized_df = predict_best_time_for_dataset(\n\u001b[32m     83\u001b[39m         df=test_df.copy(),\n\u001b[32m     84\u001b[39m         time_grid=time_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m         top_k=\u001b[32m3\u001b[39m\n\u001b[32m     91\u001b[39m     )\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Сохраняем результат\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[43mpersonalized_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpersonalized_send_times.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mГотово! Сохранено \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(personalized_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m клиентов с персональным временем\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mПример:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:3118\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3037\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3038\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3039\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3114\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python313\\Lib\\site-packages\\pandas\\io\\parquet.py:68\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m             error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA suitable version of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to import the above resulted in these errors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[31mImportError\u001b[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Замени на свой путь\n",
    "    models = full_pipeline()\n",
    "    \n",
    "    # Сохрани всё для продакшена\n",
    "    import joblib\n",
    "    joblib.dump(models, \"sto_models_v1.pkl\")\n",
    "    print(\"Модели сохранены в sto_models_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9153fc9-01eb-44e2-b4f8-549728d996eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
